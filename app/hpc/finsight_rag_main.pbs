#!/bin/bash
# ===== PBS HEADER =====
# Job name (helps grouping)
#PBS -N sam_rag_train
# Bill to project (set your real project ID; else it uses personal credits)
# # PBS -P finsight
#PBS -A finsight
# Walltime (edit)
#PBS -l walltime=12:00:00
# One GPU vnode (A40 + 36 cores + ~250GB RAM); see guide
#PBS -l select=1:ncpus=36:mpiprocs=1:ompthreads=36:mem=240gb:ngpus=1
# Merge stdout/stderr
#PBS -j oe

set -eox pipefail

# ---- Edit these in one place when need to change stacks ----
TF_MODULE="TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0" # We Pick one known-good and stick with it across runs
VENV="$HOME/.venvs/finsight-tf211-cuda117"
# --------------------------------------------------------


# ===== USER KNOBS - project tagging and paths =====
export PROJECT_TAG="finsight"
export WORK="$HOME/projects-sam/FinSight-QuantLab"
export RAG="$WORK/rag" #place where i put rag_server
export SRC="$WORK/src"                  # your code lives here
export DATA="$WORK/data"                # your small/immutable data lives here (or point to /scratch/project/CFP.../)
export OUT="$WORK/results/models"               # long-term models
#export OUT="$WORK/models/$PROJECT_TAG"
export LOG="$WORK/results/logs"                 # long-term logs
export SCRATCHFORJOB="/scratch/$USER/$PBS_JOBID"   # job-local scratch
export MODE="${MODE:-train}"            # train | sanity_gpu | sanity_cpu

# CPU/GPU env knobs for Threads & GPU behavior (tune if you know better after profiling)
export TF_NUM_INTRAOP_THREADS=${TF_NUM_INTRAOP_THREADS:-36}
export TF_NUM_INTEROP_THREADS=${TF_NUM_INTEROP_THREADS:-2}
export TF_ENABLE_ONEDNN_OPTS=${TF_ENABLE_ONEDNN_OPTS:-1}
export TF_FORCE_GPU_ALLOW_GROWTH=${TF_FORCE_GPU_ALLOW_GROWTH:-true}
# export TF_XLA_FLAGS="--tf_xla_auto_jit=2"   # enable after testing

# ===== PREP =====
mkdir -p "$SCRATCHFORJOB" "$OUT" "$LOG"
cd "$SCRATCHFORJOB"
#Stream ALL job stdout/stderr into a live file on /scratch (and still show in PBS too)
#    -> live PBS log we can `tail -f /scratch/$USER/$PBS_JOBID/pbs.live.$PBS_JOBID.out`
exec > >(stdbuf -oL tee -a "$SCRATCHFORJOB/pbs.live.$PBS_JOBID.out") 2>&1

# make sure logs come home even if the job exits early
cleanup() {
  rsync -av "$SCRATCHFORJOB"/{pbs.live.$PBS_JOBID.out,rag.log,vllm.log} "$LOG/" 2>/dev/null || true
}
trap cleanup EXIT


echo "[ENV] Host: $(hostname)"
echo "[ENV] Job : $PBS_JOBID"
echo "[ENV] Mode: $MODE"
echo "[ENV] Out : $OUT"
echo "[ENV] Log : $LOG"
echo "[ENV] Scratch: $SCRATCHFORJOB"

# ===== LOAD MODULES =====
module purge
# Load exact TF stack (no auto-detect)
module load "$TF_MODULE"

# ===== ACTIVATE VENV (create once, then reuse) =====
# One venv per TF module is safest
# Activate the matching venv
source "$VENV/bin/activate"

# Diagnostics; show GPU/TF
command -v nvidia-smi >/dev/null 2>&1 && nvidia-smi || true
python - <<'PY'
import tensorflow as tf, os
print("TF", tf.__version__, "GPUs:", tf.config.list_physical_devices("GPU"))
PY

# ===== SANITY MODES =====
if [ "$MODE" = "sanity_gpu" ]; then
  echo "[MODE] GPU sanity"
  python - <<'PY'
import tensorflow as tf
gpus = tf.config.list_physical_devices("GPU")
print("GPUs:", gpus)
if gpus:
    for g in gpus: tf.config.experimental.set_memory_growth(g, True)
    with tf.device("/GPU:0"):
        import tensorflow as tf
        a=tf.random.uniform([4096,4096]); b=tf.random.uniform([4096,4096]); _=tf.linalg.matmul(a,b).numpy()
    print("TF GPU matmul OK")
else:
    print("No GPU visible to TF")
PY
  exit 0
fi

if [ "$MODE" = "sanity_cpu" ]; then
  echo "[MODE] CPU sanity"
  python - <<'PY'
import tensorflow as tf, multiprocessing as mp
print("CPUs:", mp.cpu_count())
a=tf.random.uniform([3000,3000]); b=tf.random.uniform([3000,3000]); _=tf.linalg.matmul(a,b).numpy()
print("TF CPU matmul OK")
PY
  exit 0
fi

# ===== STAGE CODE IN → RUN → STAGE OUT =====
cd "$SCRATCHFORJOB"
rsync -a "$RAG/" ./
mkdir -p ./out

# (Example) run your trainer; adapt args as needed
# Make sure your train.py respects TF thread envs; optionally mirror in code:
#   tf.config.threading.set_intra_op_parallelism_threads(int(os.getenv("TF_NUM_INTRAOP_THREADS","36")))
#   tf.config.threading.set_inter_op_parallelism_threads(int(os.getenv("TF_NUM_INTEROP_THREADS","2")))
#set -x
#python -u ./src/train_tf_smoke.py \
#  --data_root "$DATA" \
#  --out_dir  "$SCRATCHFORJOB/out" \
#  2>&1 | tee "$SCRATCHFORJOB/train.$PROJECT_TAG.$PBS_JOBID.log"

# 1) Launch vLLM (OpenAI-compatible) on 127.0.0.1:8000
#    Choose a model you have access to (HF auth may be needed if gated).
# in short, start vLLM in background on 8000
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Meta-Llama-3.1-8B-Instruct \
  --host 127.0.0.1 --port 8000 --dtype auto \
  --max-model-len 8192 \
  > vllm.log 2>&1 &

# 2) start our FastAPI RAG server on 8080 i.e. 127.0.0.1:8080 -- (reads your code from the repo; logs to rag.log)
export VLLM_URL="http://127.0.0.1:8000/v1/chat/completions"
export VLLM_MODEL="meta-llama/Meta-Llama-3.1-8B-Instruct"
echo "vLLM: 127.0.0.1:8000  |  RAG API: 127.0.0.1:8080"
echo "VLLM_URL: $VLLM_URL | VLLM_MODEL: $VLLM_MODEL"
uvicorn rag_server:app --app-dir ./ --host 127.0.0.1 --port 8080 > rag.log 2>&1 &

wait  # keep the job alive until walltime(wait on both background pids)

set +x

# Stage results back (single source of truth under your repo)
#rsync -av "$SCRATCHFORJOB/out/" "$OUT/"
#rsync -av "$SCRATCHFORJOB/train.$PROJECT_TAG.$PBS_JOBID.log" "$LOG/"

# (Optional) Clean scratch — scratch auto-purges >~60 days anyway
# rm -rf "$SCRATCHFORJOB"
echo "[DONE] Outputs in $OUT and logs in $LOG"